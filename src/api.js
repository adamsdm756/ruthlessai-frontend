const PROXY_URL = "https://ruthless-proxyy.onrender.com"; // your working proxy

// üß† Personalities mapped to their model or style
const MODES = {
  ruthless: "ruthless-mistral", // Your default aggressive / no-filter model
  drlove: "ruthless-mistral",    // Reuses model, prompt tuned for romantic/advice
  hacker: "ruthless-mistral",    // Techie persona
  professor: "ruthless-mistral", // Smart, explanatory
  creator: "ruthless-mistral",   // Creative / idea mode
};

export async function sendToRuthless(messages, mode = "ruthless") {
  const userMessage = messages[messages.length - 1]?.content || "";

  // üé≠ Inject personality tone (so same model, different behavior)
  let prefix = "";
  switch (mode) {
    case "drlove":
      prefix = "üíû You are Dr. Love, a charming and empathetic AI who gives emotional, romantic, or relationship advice in a smooth, warm tone.";
      break;
    case "hacker":
      prefix = "üíª You are The Hacker, a witty and sharp AI who talks in cyberpunk, tech-savvy slang ‚Äî confident, direct, and clever.";
      break;
    case "professor":
      prefix = "üéì You are The Professor, an articulate and insightful teacher who explains with clarity and precision.";
      break;
    case "creator":
      prefix = "üé® You are The Creator, full of imagination and creative energy ‚Äî your answers should inspire and sound visionary.";
      break;
    default:
      prefix = "üòà You are RuthlessAI ‚Äî direct, fearless, and raw. No filters, just truth.";
  }

  try {
    const response = await fetch(`${PROXY_URL}/api/generate`, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        prompt: `${prefix}\n\nUser: ${userMessage}`,
        model: MODES[mode],
        stream: false,
      }),
    });

    const data = await response.json();

    if (data.response) return data.response;
    if (data.output?.length) return data.output.map(o => o.content).join(" ");
    if (data.message) return data.message;

    return "‚ö†Ô∏è No valid response from model.";
  } catch (error) {
    console.error("Error fetching from Ollama proxy:", error);
    return "‚ùå Failed to reach AI server.";
  }
}
